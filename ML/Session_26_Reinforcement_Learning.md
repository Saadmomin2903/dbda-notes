# Session 26 â€“ Reinforcement Learning Fundamentals

## ðŸ“š Table of Contents
1. [MDP Framework](#mdp-framework)
2. [Value Functions](#value-functions)
3. [Bellman Equations](#bellman-equations)
4. [Q-Learning](#q-learning)
5. [Policy Gradient Methods](#policy-gradient-methods)
6. [MCQs](#mcqs)
7. [Common Mistakes](#common-mistakes)
8. [One-Line Exam Facts](#one-line-exam-facts)

---

# MDP Framework

## ðŸ“˜ Markov Decision Process

**Components**:
- **S**: Set of states
- **A**: Set of actions
- **P**: Transition function P(s'|s,a)
- **R**: Reward function R(s,a)
- **Î³**: Discount factor âˆˆ [0,1]

**Markov Property**: Future depends only on current state, not history.

## ðŸ§® Goal

Find optimal policy Ï€* that maximizes expected cumulative reward:
```
Ï€* = argmax_Ï€ E[Î£_{t=0}^âˆž Î³^t r_t | Ï€]
```

**Discount factor Î³**:
- Î³ = 0: Only immediate reward matters
- Î³ â†’ 1: Future rewards important
- Î³ < 1: Ensures convergence for infinite horizon

---

# Value Functions

## ðŸ§® State Value Function

**V^Ï€(s)**: Expected return starting from state s, following policy Ï€
```
V^Ï€(s) = E_Ï€[Î£_{t=0}^âˆž Î³^t r_t | s_0 = s]
```

## ðŸ§® Action Value Function

**Q^Ï€(s,a)**: Expected return starting from s, taking action a, then following Ï€
```
Q^Ï€(s,a) = E_Ï€[Î£_{t=0}^âˆž Î³^t r_t | s_0 = s, a_0 = a]
```

**Relationship**:
```
V^Ï€(s) = Î£_a Ï€(a|s) Q^Ï€(s,a)
```

## ðŸ§® Optimal Value Functions

```
V*(s) = max_Ï€ V^Ï€(s)
Q*(s,a) = max_Ï€ Q^Ï€(s,a)
```

**Optimal policy**: Ï€*(s) = argmax_a Q*(s,a)

---

# Bellman Equations

## ðŸ§® Bellman Expectation Equation

**For V^Ï€**:
```
V^Ï€(s) = Î£_a Ï€(a|s) Î£_{s'} P(s'|s,a)[R(s,a) + Î³V^Ï€(s')]
```

**For Q^Ï€**:
```
Q^Ï€(s,a) = Î£_{s'} P(s'|s,a)[R(s,a) + Î³ Î£_{a'} Ï€(a'|s')Q^Ï€(s',a')]
```

## ðŸ§® Bellman Optimality Equation

**For V***:
```
V*(s) = max_a Î£_{s'} P(s'|s,a)[R(s,a) + Î³V*(s')]
```

**For Q***:
```
Q*(s,a) = Î£_{s'} P(s'|s,a)[R(s,a) + Î³ max_{a'} Q*(s',a')]
```

---

# Q-Learning

## ðŸ“˜ Off-Policy TD Control

**Update rule**:
```
Q(s,a) â† Q(s,a) + Î±[r + Î³ max_{a'} Q(s',a') - Q(s,a)]
```

Where:
- Î± = learning rate
- r = observed reward
- s' = next state
- Target: r + Î³ max_{a'} Q(s',a')

## ðŸ§® Q-Learning Algorithm

```
Initialize Q(s,a) arbitrarily
for each episode:
    Initialize s
    for each step:
        Choose a using Îµ-greedy policy derived from Q
        Take action a, observe r, s'
        Q(s,a) â† Q(s,a) + Î±[r + Î³ max_{a'} Q(s',a') - Q(s,a)]
        s â† s'
    until s is terminal
```

**Îµ-greedy exploration**:
```
a = {random action with probability Îµ
    {argmax_a Q(s,a) with probability 1-Îµ
```

## ðŸ§ª Python Implementation

```python
import numpy as np

class QLearningAgent:
    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):
        self.Q = np.zeros((n_states, n_actions))
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.n_actions = n_actions
    
    def choose_action(self, state):
        """Îµ-greedy action selection."""
        if np.random.random() < self.epsilon:
            return np.random.randint(self.n_actions)
        else:
            return np.argmax(self.Q[state])
    
    def update(self, state, action, reward, next_state):
        """Q-learning update."""
        td_target = reward + self.gamma * np.max(self.Q[next_state])
        td_error = td_target - self.Q[state, action]
        self.Q[state, action] += self.alpha * td_error
    
    def train(self, env, n_episodes=1000):
        """Train agent."""
        for episode in range(n_episodes):
            state = env.reset()
            done = False
            
            while not done:
                action = self.choose_action(state)
                next_state, reward, done = env.step(action)
                self.update(state, action, reward, next_state)
                state = next_state
```

## ðŸ“Š SARSA (On-Policy Alternative)

**Update rule**:
```
Q(s,a) â† Q(s,a) + Î±[r + Î³Q(s',a') - Q(s,a)]
```

**Difference from Q-Learning**:
- Q-learning: Uses max_a' Q(s',a') (off-policy)
- SARSA: Uses actual next action a' (on-policy)

---

# Policy Gradient Methods

## ðŸ“˜ Direct Policy Optimization

**Parameterize policy**: Ï€_Î¸(a|s)

**Objective**: Maximize expected return
```
J(Î¸) = E_Ï€[Î£ Î³^t r_t]
```

## ðŸ§® Policy Gradient Theorem

```
âˆ‡_Î¸ J(Î¸) = E_Ï€[âˆ‡_Î¸ log Ï€_Î¸(a|s) Q^Ï€(s,a)]
```

## ðŸ§® REINFORCE Algorithm

**Monte Carlo policy gradient**:
```
Î¸ â† Î¸ + Î± âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t) G_t
```

Where G_t = Î£_{k=t}^T Î³^{k-t} r_k (return from time t)

**With baseline** (reduce variance):
```
Î¸ â† Î¸ + Î± âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t) [G_t - b(s_t)]
```

Common baseline: b(s) = V(s)

## ðŸ§ª REINFORCE Implementation

```python
class REINFORCEAgent:
    def __init__(self, state_dim, action_dim, lr=0.01):
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)
    
    def select_action(self, state):
        """Sample action from policy."""
        probs = self.policy(torch.FloatTensor(state))
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()
        return action.item(), action_dist.log_prob(action)
    
    def update(self, log_probs, rewards, gamma=0.99):
        """REINFORCE update."""
        returns = []
        G = 0
        
        # Compute returns
        for r in reversed(rewards):
            G = r + gamma * G
            returns.insert(0, G)
        
        returns = torch.tensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        
        # Policy gradient
        loss = []
        for log_prob, G in zip(log_probs, returns):
            loss.append(-log_prob * G)
        
        loss = torch.stack(loss).sum()
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
```

---

# ðŸ”¥ MCQs

### Q1. MDP Markov property:
**Options:**
- A) Depends on all history
- B) Depends only on current state âœ“
- C) Random
- D) Not defined

**Explanation**: Future independent of past given present state.

---

### Q2. Q-Learning is:
**Options:**
- A) On-policy
- B) Off-policy âœ“
- C) Model-based
- D) Supervised

**Explanation**: Uses max Q(s',a'), not actual next action.

---

### Q3. Discount factor Î³ â†’ 1 means:
**Options:**
- A) Only immediate reward
- B) Future rewards important âœ“
- C) No discounting
- D) Random

**Explanation**: Higher Î³ gives more weight to future rewards.

---

### Q4. Policy gradient optimizes:
**Options:**
- A) Value function
- B) Policy directly âœ“
- C) Q-function
- D) Transition model

**Explanation**: Directly parameterizes and optimizes policy Ï€_Î¸.

---

### Q5. Îµ-greedy balances:
**Options:**
- A) Speed vs accuracy
- B) Exploration vs exploitation âœ“
- C) Bias vs variance
- D) Memory vs compute

**Explanation**: Îµ = exploration, 1-Îµ = exploitation.

---

# âš ï¸ Common Mistakes

1. **Confusing Q-learning and SARSA**: Q-learning off-policy, SARSA on-policy
2. **Wrong learning rate**: Too high â†’ instability, too low â†’ slow
3. **Forgetting discount factor**: Î³ critical for long-term planning
4. **Not using baseline in REINFORCE**: High variance without it
5. **Insufficient exploration**: Îµ-greedy or other exploration needed
6. **Ignoring terminal states**: Q(terminal, Â·) = 0
7. **Wrong Bellman equation**: Expectation vs optimality
8. **Not decaying Îµ**: Should decrease Îµ over time

---

# â­ One-Line Exam Facts

1. **MDP**: (S, A, P, R, Î³) framework
2. **V^Ï€(s)**: Expected return from state s following Ï€
3. **Q^Ï€(s,a)**: Expected return from (s,a) then following Ï€
4. **Bellman expectation**: V^Ï€(s) = Î£_a Ï€(a|s)[R + Î³Î£_{s'} P(s'|s,a)V^Ï€(s')]
5. **Bellman optimality**: V*(s) = max_a[R + Î³Î£_{s'} P(s'|s,a)V*(s')]
6. **Q-learning**: Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]
7. **Off-policy**: Q-learning (learns optimal policy while following different policy)
8. **On-policy**: SARSA (learns policy being followed)
9. **Policy gradient**: âˆ‡J(Î¸) = E[âˆ‡log Ï€_Î¸(a|s) Q^Ï€(s,a)]
10. **REINFORCE**: Monte Carlo policy gradient
11. **Îµ-greedy**: Exploration-exploitation tradeoff
12. **Discount Î³**: Weight on future rewards
13. **TD error**: Î´ = r + Î³V(s') - V(s)
14. **Baseline**: Reduces variance in policy gradient
15. **Optimal policy**: Ï€*(s) = argmax_a Q*(s,a)

---

**End of Session 26**
