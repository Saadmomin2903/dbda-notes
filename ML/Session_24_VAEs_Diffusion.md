# Session 24 â€“ VAEs & Diffusion Models

## ğŸ“š Table of Contents
1. [Variational Autoencoders](#variational-autoencoders)
2. [Diffusion Models](#diffusion-models)
3. [Stable Diffusion](#stable-diffusion)
4. [Comparison](#comparison)
5. [MCQs](#mcqs)
6. [Common Mistakes](#common-mistakes)
7. [One-Line Exam Facts](#one-line-exam-facts)

---

# Variational Autoencoders

## ğŸ§® Architecture

```
Encoder: x â†’ Î¼(x), ÏƒÂ²(x)
Sample: z ~ N(Î¼, ÏƒÂ²)
Decoder: z â†’ xÌ‚
```

**Key idea**: Learn continuous latent space.

## ğŸ§® Loss Function

```
L = Reconstruction_loss + KL_divergence

L = -E[log p(x|z)] + KL(q(z|x) || p(z))
```

**Reconstruction**: How well decoder reconstructs input
**KL divergence**: How close q(z|x) to prior p(z) = N(0,I)

**Expanded**:
```
KL = -0.5 Î£(1 + log ÏƒÂ² - Î¼Â² - ÏƒÂ²)
```

## ğŸ§® Reparameterization Trick

**Problem**: Can't backprop through sampling z ~ N(Î¼, ÏƒÂ²).

**Solution**: Reparameterize
```
z = Î¼ + Ïƒ âŠ™ Îµ where Îµ ~ N(0,I)
```

Now gradient flows through Î¼ and Ïƒ.

## ğŸ§ª Implementation

```python
class VAE(nn.Module):
    def encode(self, x):
        h = self.encoder(x)
        mu = self.fc_mu(h)
        log_var = self.fc_logvar(h)
        return mu, log_var
    
    def reparameterize(self, mu, log_var):
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        return self.decoder(z)
    
    def forward(self, x):
        mu, log_var = self.encode(x)
        z = self.reparameterize(mu, log_var)
        return self.decode(z), mu, log_var
    
    def loss_function(self, recon_x, x, mu, log_var):
        BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')
        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
        return BCE + KLD
```

---

# Diffusion Models

## ğŸ“˜ Core Idea

**Forward process**: Gradually add Gaussian noise
**Reverse process**: Learn to denoise step by step

## ğŸ§® Forward Diffusion

```
q(x_t | x_{t-1}) = N(x_t; âˆš(1-Î²_t) x_{t-1}, Î²_t I)
```

Over T steps (e.g., T=1000):
```
x_0 â†’ x_1 â†’ x_2 â†’ ... â†’ x_T â‰ˆ N(0,I)
```

**Closed form** (using reparameterization):
```
x_t = âˆš(á¾±_t) x_0 + âˆš(1-á¾±_t) Îµ
where á¾±_t = âˆ_{s=1}^t (1-Î²_s)
```

## ğŸ§® Reverse Diffusion

**Learn**: p_Î¸(x_{t-1} | x_t)

**Model predicts noise** Îµ_Î¸(x_t, t):
```
x_{t-1} = (1/âˆšÎ±_t)(x_t - (Î²_t/âˆš(1-á¾±_t))Îµ_Î¸(x_t, t)) + Ïƒ_t z
```

## ğŸ§® Training Objective

**Simple loss** (Predicted noise vs actual noise):
```
L = E_t,x_0,Îµ[||Îµ - Îµ_Î¸(x_t, t)||Â²]
```

**Algorithm**:
```
1. Sample x_0 from data
2. Sample t ~ Uniform(1, T)
3. Sample Îµ ~ N(0,I)
4. Compute x_t = âˆš(á¾±_t) x_0 + âˆš(1-á¾±_t) Îµ
5. Predict ÎµÌ‚ = Îµ_Î¸(x_t, t)
6. Loss = ||Îµ - ÎµÌ‚||Â²
```

## ğŸ§® Sampling

**Start from noise x_T ~ N(0,I), denoise T steps**:
```
for t = T down to 1:
    Îµ_pred = Îµ_Î¸(x_t, t)
    x_{t-1} = denoise(x_t, Îµ_pred, t)
return x_0
```

---

# Stable Diffusion

## ğŸ“˜ Latent Diffusion

**Key innovation**: Diffusion in **latent space** (not pixel space).

**Architecture**:
```
VAE Encoder: Image â†’ Latent z
Diffusion: Apply diffusion on z
VAE Decoder: Latent â†’ Image
```

**Advantages**:
- Much faster (lower dimension)
- Less compute
- Better quality

## ğŸ§® Text Conditioning

**CLIP text encoder**: Text â†’ embedding

**Cross-attention** in diffusion model:
```
Q = features from diffusion model
K, V = text embeddings from CLIP
Attention(Q, K, V) guides generation
```

## ğŸ§ª Stable Diffusion Pipeline

```python
from diffusers import StableDiffusionPipeline
import torch

pipe = StableDiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-1"
)

prompt = "A cat wearing sunglasses on the beach"
image = pipe(prompt, num_inference_steps=50).images[0]
```

## ğŸ“Š Guidance

**Classifier-Free Guidance**:
```
Îµ_guided = Îµ_unconditional + w(Îµ_conditional - Îµ_unconditional)
```

Higher w â†’ stronger prompt adherence (typical: w=7.5).

---

# Comparison

## VAE vs Diffusion

| Aspect | VAE | Diffusion |
|--------|-----|-----------|
| **Training** | Single pass | Iterative denoising |
| **Sampling** | Fast (single pass) | Slow (many steps) |
| **Quality** | Lower | Higher âœ“ |
| **Latent space** | Structured âœ“ | No explicit latent |
| **Mode coverage** | Can miss modes | Better coverage âœ“ |

## GAN vs Diffusion

| Aspect | GAN | Diffusion |
|--------|-----|-----------|
| **Training** | Unstable | Stable âœ“ |
| **Mode collapse** | Common | Rare âœ“ |
| **Sampling** | Fast âœ“ | Slow |
| **Quality** | High | Higher âœ“ |
| **Diversity** | Lower | Higher âœ“ |

---

# ğŸ”¥ MCQs

### Q1. VAE uses:
**Options:**
- A) GAN loss
- B) Reconstruction + KL divergence âœ“
- C) Only MSE
- D) Cross-entropy

**Explanation**: VAE loss = reconstruction + KL regularization.

---

### Q2. Reparameterization trick:
**Options:**
- A) z = Î¼ + Ïƒ âŠ™ Îµ âœ“
- B) z = Î¼ Ã— Ïƒ
- C) z ~ N(Î¼, ÏƒÂ²)
- D) z = Î¼ - Ïƒ

**Explanation**: Allows backprop through sampling.

---

### Q3. Diffusion forward process:
**Options:**
- A) Removes noise
- B) Adds noise gradually âœ“
- C) Generates images
- D) Trains classifier

**Explanation**: Forward process corrupts data with noise.

---

### Q4. Stable Diffusion operates in:
**Options:**
- A) Pixel space
- B) Latent space âœ“
- C) Frequency domain
- D) No space

**Explanation**: Latent diffusion for efficiency.

---

### Q5. Diffusion sampling is:
**Options:**
- A) Single step
- B) Iterative denoising âœ“
- C) Random
- D) Instant

**Explanation**: Reverse diffusion takes T steps.

---

# âš ï¸ Common Mistakes

1. **Forgetting reparameterization in VAE**: Can't train without it
2. **Wrong KL weight**: Too high â†’ blurry images
3. **Not enough diffusion steps**: Poor quality
4. **Using pixel space for diffusion**: Very slow
5. **Ignoring guidance scale**: Critical for prompt adherence
6. **Confusing forward/reverse process**: Forward adds noise, reverse removes
7. **Not normalizing inputs**: Both VAE and diffusion need normalized data

---

# â­ One-Line Exam Facts

1. **VAE loss**: Reconstruction + KL(q(z|x) || p(z))
2. **Reparameterization**: z = Î¼ + Ïƒ âŠ™ Îµ where Îµ ~ N(0,I)
3. **Forward diffusion**: Gradually add noise to x_0
4. **Reverse diffusion**: Learn to denoise from x_T to x_0
5. **Diffusion loss**: ||Îµ - Îµ_Î¸(x_t, t)||Â² (predict noise)
6. **Stable Diffusion**: Latent diffusion + CLIP text conditioning
7. **Classifier-free guidance**: Strengthen prompt adherence
8. **VAE latent space**: Continuous, structured
9. **Diffusion steps**: Typically T=1000 for training, 50+ for sampling
10. **Cross-attention**: Condition diffusion on text embeddings

---

**End of Session 24**
