# üìö Advanced Theory Notes ‚Äì Decision Analytics & Model Evaluation (PG‚ÄëDBDA, ACTS, Pune)

---
## SECTION A: Decision Analytics

### 1Ô∏è‚É£ What *Decision Analytics* means
- **Why it exists:** Business environments rarely stop at describing what happened (descriptive) or forecasting what may happen (predictive). Managers must *choose* actions; decision analytics provides a systematic, quantitative framework to turn insights into *optimal choices*.
- **How it differs:** 
  - *Descriptive* ‚Üí summarises past data (e.g., dashboards).
  - *Predictive* ‚Üí builds models to estimate future outcomes (e.g., regression, classification).
  - *Decision* ‚Üí embeds predictions into a **decision‚Äëmaking model** (utility, cost, risk) to select the *best alternative*.

### 2Ô∏è‚É£ Why prediction alone is insufficient
- **Prediction ‚â† Preference:** A model may forecast high sales for a product, but if the profit margin is low, the optimal decision could be to *not* launch it.
- **Uncertainty & Risk:** Predictions are point estimates with uncertainty; decisions must consider *distribution* of possible outcomes (e.g., via expected utility, Value‚ÄëAt‚ÄëRisk).
- **Resource Constraints:** Real‚Äëworld actions are limited by budget, time, or capacity ‚Äì factors not captured by pure prediction.

### 3Ô∏è‚É£ Decision‚Äëmaking under uncertainty & risk
| Concept | Intuition | Typical Formalisation |
|---------|-----------|-----------------------|
| **Expected Utility** | Choose action that maximises average payoff weighted by risk attitude. | \(\max_a \; \mathbb{E}[U(\text{Outcome}\mid a)]\) |
| **Risk‚ÄëAdjusted Return** | Penalise variability; e.g., Sharpe ratio. | \(\frac{\mu - r_f}{\sigma}\) |
| **Scenario Analysis** | Evaluate decisions across a set of plausible futures. | Enumerate \(\{\text{scenario}_i\}\) and compute outcomes. |
| **Stochastic Dominance** | Preference ordering without specifying utility. | First‚Äëorder: \(F_A(x) \le F_B(x)\) for all \(x\). |

#### Common MCQ Traps
- *‚ÄúThe highest predicted sales always yields the best decision.‚Äù* ‚Äì Ignores cost, risk, and constraints.
- *‚ÄúRisk‚Äëfree decisions do not need probability modeling.‚Äù* ‚Äì Even deterministic actions can have uncertain outcomes.

---
## SECTION B: Evaluating Classifiers (THEORY ONLY)

### 1Ô∏è‚É£ Why model evaluation is required
- **Why:** A classifier‚Äôs *training* performance is optimistic; we need *generalisation* assessment to ensure it will behave acceptably on unseen data and, crucially, to align with business objectives.
- **How:** Evaluation bridges the gap between statistical performance and *operational usefulness*.

### 2Ô∏è‚É£ Accuracy vs Business Usefulness
- **Accuracy** measures overall proportion of correct predictions but treats all errors equally.
- **Business usefulness** weighs errors by their *impact* (e.g., false‚Äënegative fraud detection may cost millions, while false‚Äëpositive may be a minor inconvenience).

### 3Ô∏è‚É£ Core Concepts (interpretation only)
#### Confusion Matrix
```
                Predicted
                +      -
Actual   +   TP      FN
         -   FP      TN
```
- **TP** (True Positive): Correctly predicted positive class.
- **FN** (False Negative): Missed a positive case ‚Äì often *costly* in safety‚Äëcritical domains.
- **FP** (False Positive): Incorrectly flagged negative as positive ‚Äì may waste resources.
- **TN** (True Negative): Correctly predicted negative.

#### Derived Metrics (interpretation)
| Metric | Intuition | When it matters |
|--------|-----------|-----------------|
| **Accuracy** | Overall correctness. | Balanced classes, equal error costs. |
| **Precision** | Proportion of predicted positives that are true. | High cost of *false positives* (e.g., spam filters). |
| **Recall (Sensitivity)** | Proportion of actual positives captured. | High cost of *false negatives* (e.g., disease screening). |
| **F1‚ÄëScore** | Harmonic mean of precision & recall ‚Äì balances both. | When you need a single summary under class imbalance. |

### 4Ô∏è‚É£ Cost‚ÄëSensitive Decisions & Misclassification Costs
- **Why:** Real‚Äëworld decisions assign a monetary or utility cost to each type of error (\(C_{FP}, C_{FN}\)).
- **How:** Choose the class label that minimises *expected cost*:
\[\text{Predict Positive if } \; P(\text{Positive}\mid x) \cdot C_{FN} < (1-P(\text{Positive}\mid x)) \cdot C_{FP}\]
- **Threshold Tuning:** Adjust decision threshold away from 0.5 to reflect asymmetric costs.

#### MCQ ALERT
1. *Which metric is most appropriate when the cost of missing a fraud case (FN) is far higher than flagging a legitimate transaction (FP)?* (A) Accuracy, (B) Precision, (C) Recall, (D) F1‚ÄëScore.)

---
## SECTION C: Analytical Framework (Decision‚ÄëOriented Analytics)

### 1Ô∏è‚É£ End‚Äëto‚ÄëEnd Steps
| Step | Why it matters | Typical Activities |
|------|----------------|--------------------|
| **1. Problem Definition** | Align analytics with strategic goals. | Define decision alternatives, objectives, constraints. |
| **2. Data Acquisition & Preparation** | Quality data is the foundation; garbage in ‚Üí garbage out. | Collect, clean, feature‚Äëengineer, assess uncertainty. |
| **3. Model Development** | Translate data into predictive or prescriptive insight. | Choose appropriate predictive model, calibrate, validate. |
| **4. Decision Modeling** | Convert predictions into *actionable* recommendations. | Build utility/cost functions, perform optimisation, scenario analysis. |
| **5. Implementation & Monitoring** | Real‚Äëworld impact must be measured; feedback informs refinement. | Deploy decision rule, collect outcome data, update model (feedback loop). |

### 2Ô∏è‚É£ Data ‚Üí Model ‚Üí Decision ‚Üí Outcome Feedback Loop
- **Why a loop?** Business environments evolve; static models become stale. Continuous learning ensures relevance and improves future decisions.

#### MCQ ALERT
2. *In the analytical framework, which step directly addresses *risk* associated with uncertain predictions?* (A) Data preparation, (B) Model development, (C) Decision modeling, (D) Monitoring.)

---
## SECTION D: Evaluation Philosophy

### 1Ô∏è‚É£ ‚ÄúBest model‚Äù depends on context
- **Why:** Different stakeholders value different outcomes (e.g., regulator vs. profit‚Äëmaximiser). The *optimal* model is the one that aligns with the **decision objective**, not necessarily the one with highest statistical score.

### 2Ô∏è‚É£ Trade‚Äëoff between False Positives & False Negatives
- **Why:** Adjusting the decision threshold moves the operating point along the ROC curve; the optimal point balances *business cost* of each error type.
- **How to visualise:** Cost curve or profit curve ‚Äì plot expected profit vs. threshold.

#### MCQ ALERT
3. *A classifier with 99‚ÄØ% accuracy on a dataset where 1‚ÄØ% are positives suffers from which paradox?* (A) Overfitting, (B) Accuracy paradox, (C) Class‚Äëimbalance, (D) None of the above.)

---
## üìä Comparison Table ‚Äì Metrics vs. Business Goals
| Business Goal | Preferred Metric(s) | Rationale |
|---------------|--------------------|-----------|
| Minimise costly false alarms (e.g., spam) | Precision, Cost‚Äësensitive loss | Penalises FP heavily. |
| Detect rare events (e.g., fraud) | Recall, F1‚ÄëScore, ROC‚ÄëAUC | Rewards capturing positives. |
| Balanced performance on balanced data | Accuracy, ROC‚ÄëAUC | Errors equally weighted. |
| Maximise overall profit | Expected Cost, Profit Curve | Directly incorporates monetary impact. |

---
## üìù 15 Examiner‚ÄëStyle Conceptual MCQs (with explanations)
1. **Why is prediction alone insufficient for decision making?**
   - *Explanation:* Prediction provides *what may happen* but does not encode *preferences* (costs, utilities) or *constraints* needed to choose an action.
2. **In a confusion matrix, which cell directly contributes to *Recall*?**
   - *Explanation:* Recall = TP / (TP + FN); only TP and FN matter.
3. **What does the *accuracy paradox* illustrate?**
   - *Explanation:* High overall accuracy can mask poor performance on the minority class when data is imbalanced.
4. **When should you prefer the F1‚ÄëScore over Accuracy?**
   - *Explanation:* When class distribution is skewed and you need a balance between Precision and Recall.
5. **Which decision‚Äëanalytic concept explicitly incorporates the decision‚Äëmaker‚Äôs risk attitude?**
   - *Explanation:* Expected Utility Theory.
6. **How does *cost‚Äësensitive learning* differ from standard classification?**
   - *Explanation:* It weights errors by their real‚Äëworld costs rather than treating all mistakes equally.
7. **What is the primary purpose of *scenario analysis* in decision analytics?**
   - *Explanation:* To evaluate how decisions perform under a set of plausible future states.
8. **Why might a model with higher ROC‚ÄëAUC be unsuitable for a particular business problem?**
   - *Explanation:* ROC‚ÄëAUC ignores actual cost/benefit structure; a model with lower AUC but better alignment to cost may be preferable.
9. **Which metric is most appropriate for evaluating a medical test where missing a disease is catastrophic?**
   - *Explanation:* Recall (Sensitivity) because false negatives are extremely costly.
10. **What does *Stochastic Dominance* allow you to compare without specifying a utility function?**
    - *Explanation:* It provides a partial ordering of distributions based on risk‚Äëaverse preferences.
11. **In decision modeling, what role does the *utility function* play?**
    - *Explanation:* It translates outcomes into a scalar measure of desirability, reflecting stakeholder preferences.
12. **Why is the *feedback loop* essential in analytical frameworks?**
    - *Explanation:* It enables model updating and continuous improvement as real outcomes become available.
13. **Which error type is typically more concerning in credit‚Äëcard fraud detection?**
    - *Explanation:* False Negatives (missed fraud) because they incur direct monetary loss.
14. **How does *threshold tuning* affect the trade‚Äëoff between FP and FN?**
    - *Explanation:* Raising the threshold reduces FP but increases FN; lowering does the opposite.
15. **What is the key limitation of using *accuracy* as the sole evaluation metric in imbalanced datasets?**
    - *Explanation:* It can be misleading; a classifier that always predicts the majority class can achieve high accuracy while being useless for the minority class.

---
## üìö Further Reading (concise list)
- *Decision Analysis for the Real World* ‚Äì Clemen & Reilly (chapters on utility & risk).
- *The Elements of Statistical Learning* ‚Äì Hastie, Tibshirani, Friedman (section on model evaluation).
- *Machine Learning: A Probabilistic Perspective* ‚Äì Kevin Murphy (cost‚Äësensitive learning).
- *Data‚ÄëDriven Decision Making* ‚Äì Provost & Fawcett (framework & feedback loop).
- *An Introduction to ROC Analysis* ‚Äì Fawcett (2006).

---
*End of notes.*
